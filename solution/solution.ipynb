{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af08b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 FINANCIAL FORECASTING COMPETITION PIPELINE\n",
      "Version: Final Competition Ready\n",
      "============================================================\n",
      "🚀 PREPARING COMPETITION SUBMISSION\n",
      "============================================================\n",
      "📥 Loading training data...\n",
      "✅ Train candles: (25942, 7)\n",
      "✅ Train news: (27455, 4)\n",
      "📅 Date range: 2020-06-19 to 2025-09-08\n",
      "🎯 Tickers: ['AFLT' 'ALRS' 'CHMF' 'GAZP' 'GMKN' 'LKOH' 'MAGN' 'MGNT' 'MOEX' 'MTSS'\n",
      " 'NVTK' 'PHOR' 'PLZL' 'ROSN' 'RUAL' 'SBER' 'SIBN' 'T' 'VTBR']\n",
      "\n",
      "🔄 Processing data with preprocessor...\n",
      "🚀 Processing training data for returns prediction...\n",
      "🔄 Loading news embedding model...\n",
      "✅ News model loaded successfully!\n",
      "📰 Processing news data with compressed embeddings...\n",
      "🔧 Compressing embeddings from 384 to 64 dimensions...\n",
      "✅ Generated compressed embeddings for 1820 news dates\n",
      "\n",
      "📊 Processing ticker: AFLT\n",
      "✅ AFLT, return_1d: Already stationary (p-value: 0.0000)\n",
      "✅ AFLT, return_2d: Already stationary (p-value: 0.0000)\n",
      "✅ AFLT, return_3d: Already stationary (p-value: 0.0000)\n",
      "✅ AFLT, return_4d: Already stationary (p-value: 0.0000)\n",
      "✅ AFLT, return_5d: Already stationary (p-value: 0.0000)\n",
      "✅ AFLT, return_6d: Already stationary (p-value: 0.0000)\n",
      "✅ AFLT, return_7d: Already stationary (p-value: 0.0000)\n",
      "✅ AFLT, return_8d: Already stationary (p-value: 0.0000)\n",
      "✅ AFLT, return_9d: Already stationary (p-value: 0.0000)\n",
      "✅ AFLT, return_10d: Already stationary (p-value: 0.0000)\n",
      "✅ AFLT, return_11d: Already stationary (p-value: 0.0000)\n",
      "✅ AFLT, return_12d: Already stationary (p-value: 0.0000)\n",
      "✅ AFLT, return_13d: Already stationary (p-value: 0.0000)\n",
      "✅ AFLT, return_14d: Already stationary (p-value: 0.0000)\n",
      "✅ AFLT, return_15d: Already stationary (p-value: 0.0000)\n",
      "✅ AFLT, return_16d: Already stationary (p-value: 0.0000)\n",
      "✅ AFLT, return_17d: Already stationary (p-value: 0.0000)\n",
      "✅ AFLT, return_18d: Already stationary (p-value: 0.0000)\n",
      "✅ AFLT, return_19d: Already stationary (p-value: 0.0000)\n",
      "✅ AFLT, return_20d: Already stationary (p-value: 0.0000)\n",
      "🔗 Adding compressed news embeddings to dataset...\n",
      "✅ Added 64 compressed news embeddings to 1691 rows\n",
      "📅 Filtered out 681 rows before 2022-05-01 00:00:00\n",
      "\n",
      "📊 Processing ticker: ALRS\n",
      "✅ ALRS, return_1d: Already stationary (p-value: 0.0000)\n",
      "✅ ALRS, return_2d: Already stationary (p-value: 0.0000)\n",
      "✅ ALRS, return_3d: Already stationary (p-value: 0.0000)\n",
      "✅ ALRS, return_4d: Already stationary (p-value: 0.0000)\n",
      "✅ ALRS, return_5d: Already stationary (p-value: 0.0000)\n",
      "✅ ALRS, return_6d: Already stationary (p-value: 0.0000)\n",
      "✅ ALRS, return_7d: Already stationary (p-value: 0.0000)\n",
      "✅ ALRS, return_8d: Already stationary (p-value: 0.0000)\n",
      "✅ ALRS, return_9d: Already stationary (p-value: 0.0000)\n",
      "✅ ALRS, return_10d: Already stationary (p-value: 0.0000)\n",
      "✅ ALRS, return_11d: Already stationary (p-value: 0.0000)\n",
      "✅ ALRS, return_12d: Already stationary (p-value: 0.0000)\n",
      "✅ ALRS, return_13d: Already stationary (p-value: 0.0000)\n",
      "✅ ALRS, return_14d: Already stationary (p-value: 0.0000)\n",
      "✅ ALRS, return_15d: Already stationary (p-value: 0.0000)\n",
      "✅ ALRS, return_16d: Already stationary (p-value: 0.0000)\n",
      "✅ ALRS, return_17d: Already stationary (p-value: 0.0000)\n",
      "✅ ALRS, return_18d: Already stationary (p-value: 0.0000)\n",
      "✅ ALRS, return_19d: Already stationary (p-value: 0.0000)\n",
      "✅ ALRS, return_20d: Already stationary (p-value: 0.0000)\n",
      "🔗 Adding compressed news embeddings to dataset...\n",
      "✅ Added 64 compressed news embeddings to 1691 rows\n",
      "📅 Filtered out 681 rows before 2022-05-01 00:00:00\n",
      "\n",
      "📊 Processing ticker: CHMF\n",
      "✅ CHMF, return_1d: Already stationary (p-value: 0.0000)\n",
      "✅ CHMF, return_2d: Already stationary (p-value: 0.0000)\n",
      "✅ CHMF, return_3d: Already stationary (p-value: 0.0000)\n",
      "✅ CHMF, return_4d: Already stationary (p-value: 0.0000)\n",
      "✅ CHMF, return_5d: Already stationary (p-value: 0.0000)\n",
      "✅ CHMF, return_6d: Already stationary (p-value: 0.0000)\n",
      "✅ CHMF, return_7d: Already stationary (p-value: 0.0000)\n",
      "✅ CHMF, return_8d: Already stationary (p-value: 0.0000)\n",
      "✅ CHMF, return_9d: Already stationary (p-value: 0.0000)\n",
      "✅ CHMF, return_10d: Already stationary (p-value: 0.0000)\n",
      "✅ CHMF, return_11d: Already stationary (p-value: 0.0000)\n",
      "✅ CHMF, return_12d: Already stationary (p-value: 0.0000)\n",
      "✅ CHMF, return_13d: Already stationary (p-value: 0.0000)\n",
      "✅ CHMF, return_14d: Already stationary (p-value: 0.0000)\n",
      "✅ CHMF, return_15d: Already stationary (p-value: 0.0000)\n",
      "✅ CHMF, return_16d: Already stationary (p-value: 0.0000)\n",
      "✅ CHMF, return_17d: Already stationary (p-value: 0.0000)\n",
      "✅ CHMF, return_18d: Already stationary (p-value: 0.0000)\n",
      "✅ CHMF, return_19d: Already stationary (p-value: 0.0000)\n",
      "✅ CHMF, return_20d: Already stationary (p-value: 0.0000)\n",
      "🔗 Adding compressed news embeddings to dataset...\n",
      "✅ Added 64 compressed news embeddings to 1691 rows\n",
      "📅 Filtered out 681 rows before 2022-05-01 00:00:00\n",
      "\n",
      "📊 Processing ticker: GAZP\n",
      "✅ GAZP, return_1d: Already stationary (p-value: 0.0000)\n",
      "✅ GAZP, return_2d: Already stationary (p-value: 0.0000)\n",
      "✅ GAZP, return_3d: Already stationary (p-value: 0.0000)\n",
      "✅ GAZP, return_4d: Already stationary (p-value: 0.0000)\n",
      "✅ GAZP, return_5d: Already stationary (p-value: 0.0000)\n",
      "✅ GAZP, return_6d: Already stationary (p-value: 0.0000)\n",
      "✅ GAZP, return_7d: Already stationary (p-value: 0.0000)\n",
      "✅ GAZP, return_8d: Already stationary (p-value: 0.0000)\n",
      "✅ GAZP, return_9d: Already stationary (p-value: 0.0000)\n",
      "✅ GAZP, return_10d: Already stationary (p-value: 0.0000)\n",
      "✅ GAZP, return_11d: Already stationary (p-value: 0.0000)\n",
      "✅ GAZP, return_12d: Already stationary (p-value: 0.0000)\n",
      "✅ GAZP, return_13d: Already stationary (p-value: 0.0000)\n",
      "✅ GAZP, return_14d: Already stationary (p-value: 0.0000)\n",
      "✅ GAZP, return_15d: Already stationary (p-value: 0.0000)\n",
      "✅ GAZP, return_16d: Already stationary (p-value: 0.0000)\n",
      "✅ GAZP, return_17d: Already stationary (p-value: 0.0000)\n",
      "✅ GAZP, return_18d: Already stationary (p-value: 0.0000)\n",
      "✅ GAZP, return_19d: Already stationary (p-value: 0.0000)\n",
      "✅ GAZP, return_20d: Already stationary (p-value: 0.0000)\n",
      "🔗 Adding compressed news embeddings to dataset...\n",
      "✅ Added 64 compressed news embeddings to 1691 rows\n",
      "📅 Filtered out 681 rows before 2022-05-01 00:00:00\n",
      "\n",
      "📊 Processing ticker: GMKN\n",
      "✅ GMKN, return_1d: Already stationary (p-value: 0.0000)\n",
      "✅ GMKN, return_2d: Already stationary (p-value: 0.0000)\n",
      "✅ GMKN, return_3d: Already stationary (p-value: 0.0000)\n",
      "✅ GMKN, return_4d: Already stationary (p-value: 0.0000)\n",
      "✅ GMKN, return_5d: Already stationary (p-value: 0.0000)\n",
      "✅ GMKN, return_6d: Already stationary (p-value: 0.0000)\n",
      "✅ GMKN, return_7d: Already stationary (p-value: 0.0000)\n",
      "✅ GMKN, return_8d: Already stationary (p-value: 0.0000)\n",
      "✅ GMKN, return_9d: Already stationary (p-value: 0.0000)\n",
      "✅ GMKN, return_10d: Already stationary (p-value: 0.0000)\n",
      "✅ GMKN, return_11d: Already stationary (p-value: 0.0000)\n",
      "✅ GMKN, return_12d: Already stationary (p-value: 0.0000)\n",
      "✅ GMKN, return_13d: Already stationary (p-value: 0.0000)\n",
      "✅ GMKN, return_14d: Already stationary (p-value: 0.0000)\n",
      "✅ GMKN, return_15d: Already stationary (p-value: 0.0000)\n",
      "✅ GMKN, return_16d: Already stationary (p-value: 0.0000)\n",
      "✅ GMKN, return_17d: Already stationary (p-value: 0.0000)\n",
      "✅ GMKN, return_18d: Already stationary (p-value: 0.0000)\n",
      "✅ GMKN, return_19d: Already stationary (p-value: 0.0000)\n",
      "✅ GMKN, return_20d: Already stationary (p-value: 0.0000)\n",
      "🔗 Adding compressed news embeddings to dataset...\n",
      "✅ Added 64 compressed news embeddings to 1691 rows\n",
      "📅 Filtered out 681 rows before 2022-05-01 00:00:00\n",
      "\n",
      "📊 Processing ticker: LKOH\n",
      "✅ LKOH, return_1d: Already stationary (p-value: 0.0000)\n",
      "✅ LKOH, return_2d: Already stationary (p-value: 0.0000)\n",
      "✅ LKOH, return_3d: Already stationary (p-value: 0.0000)\n",
      "✅ LKOH, return_4d: Already stationary (p-value: 0.0000)\n",
      "✅ LKOH, return_5d: Already stationary (p-value: 0.0000)\n",
      "✅ LKOH, return_6d: Already stationary (p-value: 0.0000)\n",
      "✅ LKOH, return_7d: Already stationary (p-value: 0.0000)\n",
      "✅ LKOH, return_8d: Already stationary (p-value: 0.0000)\n",
      "✅ LKOH, return_9d: Already stationary (p-value: 0.0000)\n",
      "✅ LKOH, return_10d: Already stationary (p-value: 0.0000)\n",
      "✅ LKOH, return_11d: Already stationary (p-value: 0.0000)\n",
      "✅ LKOH, return_12d: Already stationary (p-value: 0.0000)\n",
      "✅ LKOH, return_13d: Already stationary (p-value: 0.0000)\n",
      "✅ LKOH, return_14d: Already stationary (p-value: 0.0000)\n",
      "✅ LKOH, return_15d: Already stationary (p-value: 0.0000)\n",
      "✅ LKOH, return_16d: Already stationary (p-value: 0.0000)\n",
      "✅ LKOH, return_17d: Already stationary (p-value: 0.0000)\n",
      "✅ LKOH, return_18d: Already stationary (p-value: 0.0000)\n",
      "✅ LKOH, return_19d: Already stationary (p-value: 0.0000)\n",
      "✅ LKOH, return_20d: Already stationary (p-value: 0.0000)\n",
      "🔗 Adding compressed news embeddings to dataset...\n",
      "✅ Added 64 compressed news embeddings to 1691 rows\n",
      "📅 Filtered out 681 rows before 2022-05-01 00:00:00\n",
      "\n",
      "📊 Processing ticker: MAGN\n",
      "✅ MAGN, return_1d: Already stationary (p-value: 0.0000)\n",
      "✅ MAGN, return_2d: Already stationary (p-value: 0.0000)\n",
      "✅ MAGN, return_3d: Already stationary (p-value: 0.0000)\n",
      "✅ MAGN, return_4d: Already stationary (p-value: 0.0000)\n",
      "✅ MAGN, return_5d: Already stationary (p-value: 0.0000)\n",
      "✅ MAGN, return_6d: Already stationary (p-value: 0.0000)\n",
      "✅ MAGN, return_7d: Already stationary (p-value: 0.0000)\n",
      "✅ MAGN, return_8d: Already stationary (p-value: 0.0000)\n",
      "✅ MAGN, return_9d: Already stationary (p-value: 0.0000)\n",
      "✅ MAGN, return_10d: Already stationary (p-value: 0.0000)\n",
      "✅ MAGN, return_11d: Already stationary (p-value: 0.0000)\n",
      "✅ MAGN, return_12d: Already stationary (p-value: 0.0000)\n",
      "✅ MAGN, return_13d: Already stationary (p-value: 0.0000)\n",
      "✅ MAGN, return_14d: Already stationary (p-value: 0.0000)\n",
      "✅ MAGN, return_15d: Already stationary (p-value: 0.0000)\n",
      "✅ MAGN, return_16d: Already stationary (p-value: 0.0000)\n",
      "✅ MAGN, return_17d: Already stationary (p-value: 0.0000)\n",
      "✅ MAGN, return_18d: Already stationary (p-value: 0.0000)\n",
      "✅ MAGN, return_19d: Already stationary (p-value: 0.0000)\n",
      "✅ MAGN, return_20d: Already stationary (p-value: 0.0000)\n",
      "🔗 Adding compressed news embeddings to dataset...\n",
      "✅ Added 64 compressed news embeddings to 1691 rows\n",
      "📅 Filtered out 681 rows before 2022-05-01 00:00:00\n",
      "\n",
      "📊 Processing ticker: MGNT\n",
      "✅ MGNT, return_1d: Already stationary (p-value: 0.0000)\n",
      "✅ MGNT, return_2d: Already stationary (p-value: 0.0000)\n",
      "✅ MGNT, return_3d: Already stationary (p-value: 0.0000)\n",
      "✅ MGNT, return_4d: Already stationary (p-value: 0.0000)\n",
      "✅ MGNT, return_5d: Already stationary (p-value: 0.0000)\n",
      "✅ MGNT, return_6d: Already stationary (p-value: 0.0000)\n",
      "✅ MGNT, return_7d: Already stationary (p-value: 0.0000)\n",
      "✅ MGNT, return_8d: Already stationary (p-value: 0.0000)\n",
      "✅ MGNT, return_9d: Already stationary (p-value: 0.0000)\n",
      "✅ MGNT, return_10d: Already stationary (p-value: 0.0000)\n",
      "✅ MGNT, return_11d: Already stationary (p-value: 0.0000)\n",
      "✅ MGNT, return_12d: Already stationary (p-value: 0.0000)\n",
      "✅ MGNT, return_13d: Already stationary (p-value: 0.0000)\n",
      "✅ MGNT, return_14d: Already stationary (p-value: 0.0000)\n",
      "✅ MGNT, return_15d: Already stationary (p-value: 0.0000)\n",
      "✅ MGNT, return_16d: Already stationary (p-value: 0.0000)\n",
      "✅ MGNT, return_17d: Already stationary (p-value: 0.0000)\n",
      "✅ MGNT, return_18d: Already stationary (p-value: 0.0000)\n",
      "✅ MGNT, return_19d: Already stationary (p-value: 0.0000)\n",
      "✅ MGNT, return_20d: Already stationary (p-value: 0.0000)\n",
      "🔗 Adding compressed news embeddings to dataset...\n",
      "✅ Added 64 compressed news embeddings to 1691 rows\n",
      "📅 Filtered out 681 rows before 2022-05-01 00:00:00\n",
      "\n",
      "📊 Processing ticker: MOEX\n",
      "✅ MOEX, return_1d: Already stationary (p-value: 0.0000)\n",
      "✅ MOEX, return_2d: Already stationary (p-value: 0.0000)\n",
      "✅ MOEX, return_3d: Already stationary (p-value: 0.0000)\n",
      "✅ MOEX, return_4d: Already stationary (p-value: 0.0000)\n",
      "✅ MOEX, return_5d: Already stationary (p-value: 0.0000)\n",
      "✅ MOEX, return_6d: Already stationary (p-value: 0.0000)\n",
      "✅ MOEX, return_7d: Already stationary (p-value: 0.0000)\n",
      "✅ MOEX, return_8d: Already stationary (p-value: 0.0000)\n",
      "✅ MOEX, return_9d: Already stationary (p-value: 0.0000)\n",
      "✅ MOEX, return_10d: Already stationary (p-value: 0.0000)\n",
      "✅ MOEX, return_11d: Already stationary (p-value: 0.0000)\n",
      "✅ MOEX, return_12d: Already stationary (p-value: 0.0000)\n",
      "✅ MOEX, return_13d: Already stationary (p-value: 0.0000)\n",
      "✅ MOEX, return_14d: Already stationary (p-value: 0.0000)\n",
      "✅ MOEX, return_15d: Already stationary (p-value: 0.0000)\n",
      "✅ MOEX, return_16d: Already stationary (p-value: 0.0000)\n",
      "✅ MOEX, return_17d: Already stationary (p-value: 0.0000)\n",
      "✅ MOEX, return_18d: Already stationary (p-value: 0.0000)\n",
      "✅ MOEX, return_19d: Already stationary (p-value: 0.0000)\n",
      "✅ MOEX, return_20d: Already stationary (p-value: 0.0000)\n",
      "🔗 Adding compressed news embeddings to dataset...\n",
      "✅ Added 64 compressed news embeddings to 1691 rows\n",
      "📅 Filtered out 681 rows before 2022-05-01 00:00:00\n",
      "\n",
      "📊 Processing ticker: MTSS\n",
      "✅ MTSS, return_1d: Already stationary (p-value: 0.0000)\n",
      "✅ MTSS, return_2d: Already stationary (p-value: 0.0000)\n",
      "✅ MTSS, return_3d: Already stationary (p-value: 0.0000)\n",
      "✅ MTSS, return_4d: Already stationary (p-value: 0.0000)\n",
      "✅ MTSS, return_5d: Already stationary (p-value: 0.0000)\n",
      "✅ MTSS, return_6d: Already stationary (p-value: 0.0000)\n",
      "✅ MTSS, return_7d: Already stationary (p-value: 0.0000)\n",
      "✅ MTSS, return_8d: Already stationary (p-value: 0.0000)\n",
      "✅ MTSS, return_9d: Already stationary (p-value: 0.0000)\n",
      "✅ MTSS, return_10d: Already stationary (p-value: 0.0000)\n",
      "✅ MTSS, return_11d: Already stationary (p-value: 0.0000)\n",
      "✅ MTSS, return_12d: Already stationary (p-value: 0.0000)\n",
      "✅ MTSS, return_13d: Already stationary (p-value: 0.0000)\n",
      "✅ MTSS, return_14d: Already stationary (p-value: 0.0000)\n",
      "✅ MTSS, return_15d: Already stationary (p-value: 0.0000)\n",
      "✅ MTSS, return_16d: Already stationary (p-value: 0.0000)\n",
      "✅ MTSS, return_17d: Already stationary (p-value: 0.0000)\n",
      "✅ MTSS, return_18d: Already stationary (p-value: 0.0000)\n",
      "✅ MTSS, return_19d: Already stationary (p-value: 0.0000)\n",
      "✅ MTSS, return_20d: Already stationary (p-value: 0.0000)\n",
      "🔗 Adding compressed news embeddings to dataset...\n",
      "✅ Added 64 compressed news embeddings to 1691 rows\n",
      "📅 Filtered out 681 rows before 2022-05-01 00:00:00\n",
      "\n",
      "📊 Processing ticker: NVTK\n",
      "✅ NVTK, return_1d: Already stationary (p-value: 0.0000)\n",
      "✅ NVTK, return_2d: Already stationary (p-value: 0.0000)\n",
      "✅ NVTK, return_3d: Already stationary (p-value: 0.0000)\n",
      "✅ NVTK, return_4d: Already stationary (p-value: 0.0000)\n",
      "✅ NVTK, return_5d: Already stationary (p-value: 0.0000)\n",
      "✅ NVTK, return_6d: Already stationary (p-value: 0.0000)\n",
      "✅ NVTK, return_7d: Already stationary (p-value: 0.0000)\n",
      "✅ NVTK, return_8d: Already stationary (p-value: 0.0000)\n",
      "✅ NVTK, return_9d: Already stationary (p-value: 0.0000)\n",
      "✅ NVTK, return_10d: Already stationary (p-value: 0.0000)\n",
      "✅ NVTK, return_11d: Already stationary (p-value: 0.0000)\n",
      "✅ NVTK, return_12d: Already stationary (p-value: 0.0000)\n",
      "✅ NVTK, return_13d: Already stationary (p-value: 0.0000)\n",
      "✅ NVTK, return_14d: Already stationary (p-value: 0.0000)\n",
      "✅ NVTK, return_15d: Already stationary (p-value: 0.0000)\n",
      "✅ NVTK, return_16d: Already stationary (p-value: 0.0000)\n",
      "✅ NVTK, return_17d: Already stationary (p-value: 0.0000)\n",
      "✅ NVTK, return_18d: Already stationary (p-value: 0.0000)\n",
      "✅ NVTK, return_19d: Already stationary (p-value: 0.0000)\n",
      "✅ NVTK, return_20d: Already stationary (p-value: 0.0000)\n",
      "🔗 Adding compressed news embeddings to dataset...\n",
      "✅ Added 64 compressed news embeddings to 1691 rows\n",
      "📅 Filtered out 681 rows before 2022-05-01 00:00:00\n",
      "\n",
      "📊 Processing ticker: PHOR\n",
      "✅ PHOR, return_1d: Already stationary (p-value: 0.0000)\n",
      "✅ PHOR, return_2d: Already stationary (p-value: 0.0000)\n",
      "✅ PHOR, return_3d: Already stationary (p-value: 0.0000)\n",
      "✅ PHOR, return_4d: Already stationary (p-value: 0.0000)\n",
      "✅ PHOR, return_5d: Already stationary (p-value: 0.0000)\n",
      "✅ PHOR, return_6d: Already stationary (p-value: 0.0000)\n",
      "✅ PHOR, return_7d: Already stationary (p-value: 0.0000)\n",
      "✅ PHOR, return_8d: Already stationary (p-value: 0.0000)\n",
      "✅ PHOR, return_9d: Already stationary (p-value: 0.0000)\n",
      "✅ PHOR, return_10d: Already stationary (p-value: 0.0000)\n",
      "✅ PHOR, return_11d: Already stationary (p-value: 0.0000)\n",
      "✅ PHOR, return_12d: Already stationary (p-value: 0.0000)\n",
      "✅ PHOR, return_13d: Already stationary (p-value: 0.0000)\n",
      "✅ PHOR, return_14d: Already stationary (p-value: 0.0000)\n",
      "✅ PHOR, return_15d: Already stationary (p-value: 0.0000)\n",
      "✅ PHOR, return_16d: Already stationary (p-value: 0.0000)\n",
      "✅ PHOR, return_17d: Already stationary (p-value: 0.0000)\n",
      "✅ PHOR, return_18d: Already stationary (p-value: 0.0000)\n",
      "✅ PHOR, return_19d: Already stationary (p-value: 0.0000)\n",
      "✅ PHOR, return_20d: Already stationary (p-value: 0.0000)\n",
      "🔗 Adding compressed news embeddings to dataset...\n",
      "✅ Added 64 compressed news embeddings to 1691 rows\n",
      "📅 Filtered out 681 rows before 2022-05-01 00:00:00\n",
      "\n",
      "📊 Processing ticker: PLZL\n",
      "✅ PLZL, return_1d: Already stationary (p-value: 0.0000)\n",
      "✅ PLZL, return_2d: Already stationary (p-value: 0.0000)\n",
      "✅ PLZL, return_3d: Already stationary (p-value: 0.0000)\n",
      "✅ PLZL, return_4d: Already stationary (p-value: 0.0000)\n",
      "✅ PLZL, return_5d: Already stationary (p-value: 0.0000)\n",
      "✅ PLZL, return_6d: Already stationary (p-value: 0.0000)\n",
      "✅ PLZL, return_7d: Already stationary (p-value: 0.0000)\n",
      "✅ PLZL, return_8d: Already stationary (p-value: 0.0000)\n",
      "✅ PLZL, return_9d: Already stationary (p-value: 0.0000)\n",
      "✅ PLZL, return_10d: Already stationary (p-value: 0.0000)\n",
      "✅ PLZL, return_11d: Already stationary (p-value: 0.0000)\n",
      "✅ PLZL, return_12d: Already stationary (p-value: 0.0000)\n",
      "✅ PLZL, return_13d: Already stationary (p-value: 0.0000)\n",
      "✅ PLZL, return_14d: Already stationary (p-value: 0.0000)\n",
      "✅ PLZL, return_15d: Already stationary (p-value: 0.0000)\n",
      "✅ PLZL, return_16d: Already stationary (p-value: 0.0000)\n",
      "✅ PLZL, return_17d: Already stationary (p-value: 0.0000)\n",
      "✅ PLZL, return_18d: Already stationary (p-value: 0.0000)\n",
      "✅ PLZL, return_19d: Already stationary (p-value: 0.0000)\n",
      "✅ PLZL, return_20d: Already stationary (p-value: 0.0000)\n",
      "🔗 Adding compressed news embeddings to dataset...\n",
      "✅ Added 64 compressed news embeddings to 1691 rows\n",
      "📅 Filtered out 681 rows before 2022-05-01 00:00:00\n",
      "\n",
      "📊 Processing ticker: ROSN\n",
      "✅ ROSN, return_1d: Already stationary (p-value: 0.0000)\n",
      "✅ ROSN, return_2d: Already stationary (p-value: 0.0000)\n",
      "✅ ROSN, return_3d: Already stationary (p-value: 0.0000)\n",
      "✅ ROSN, return_4d: Already stationary (p-value: 0.0000)\n",
      "✅ ROSN, return_5d: Already stationary (p-value: 0.0000)\n",
      "✅ ROSN, return_6d: Already stationary (p-value: 0.0000)\n",
      "✅ ROSN, return_7d: Already stationary (p-value: 0.0000)\n",
      "✅ ROSN, return_8d: Already stationary (p-value: 0.0000)\n",
      "✅ ROSN, return_9d: Already stationary (p-value: 0.0000)\n",
      "✅ ROSN, return_10d: Already stationary (p-value: 0.0000)\n",
      "✅ ROSN, return_11d: Already stationary (p-value: 0.0000)\n",
      "✅ ROSN, return_12d: Already stationary (p-value: 0.0000)\n",
      "✅ ROSN, return_13d: Already stationary (p-value: 0.0000)\n",
      "✅ ROSN, return_14d: Already stationary (p-value: 0.0000)\n",
      "✅ ROSN, return_15d: Already stationary (p-value: 0.0000)\n",
      "✅ ROSN, return_16d: Already stationary (p-value: 0.0000)\n",
      "✅ ROSN, return_17d: Already stationary (p-value: 0.0000)\n",
      "✅ ROSN, return_18d: Already stationary (p-value: 0.0000)\n",
      "✅ ROSN, return_19d: Already stationary (p-value: 0.0000)\n",
      "✅ ROSN, return_20d: Already stationary (p-value: 0.0000)\n",
      "🔗 Adding compressed news embeddings to dataset...\n",
      "✅ Added 64 compressed news embeddings to 1691 rows\n",
      "📅 Filtered out 681 rows before 2022-05-01 00:00:00\n",
      "\n",
      "📊 Processing ticker: RUAL\n",
      "✅ RUAL, return_1d: Already stationary (p-value: 0.0000)\n",
      "✅ RUAL, return_2d: Already stationary (p-value: 0.0000)\n",
      "✅ RUAL, return_3d: Already stationary (p-value: 0.0000)\n",
      "✅ RUAL, return_4d: Already stationary (p-value: 0.0000)\n",
      "✅ RUAL, return_5d: Already stationary (p-value: 0.0000)\n",
      "✅ RUAL, return_6d: Already stationary (p-value: 0.0000)\n",
      "✅ RUAL, return_7d: Already stationary (p-value: 0.0000)\n",
      "✅ RUAL, return_8d: Already stationary (p-value: 0.0000)\n",
      "✅ RUAL, return_9d: Already stationary (p-value: 0.0000)\n",
      "✅ RUAL, return_10d: Already stationary (p-value: 0.0000)\n",
      "✅ RUAL, return_11d: Already stationary (p-value: 0.0000)\n",
      "✅ RUAL, return_12d: Already stationary (p-value: 0.0000)\n",
      "✅ RUAL, return_13d: Already stationary (p-value: 0.0000)\n",
      "✅ RUAL, return_14d: Already stationary (p-value: 0.0000)\n",
      "✅ RUAL, return_15d: Already stationary (p-value: 0.0000)\n",
      "✅ RUAL, return_16d: Already stationary (p-value: 0.0000)\n",
      "✅ RUAL, return_17d: Already stationary (p-value: 0.0000)\n",
      "✅ RUAL, return_18d: Already stationary (p-value: 0.0000)\n",
      "✅ RUAL, return_19d: Already stationary (p-value: 0.0000)\n",
      "✅ RUAL, return_20d: Already stationary (p-value: 0.0000)\n",
      "🔗 Adding compressed news embeddings to dataset...\n",
      "✅ Added 64 compressed news embeddings to 1691 rows\n",
      "📅 Filtered out 681 rows before 2022-05-01 00:00:00\n",
      "\n",
      "📊 Processing ticker: SBER\n",
      "✅ SBER, return_1d: Already stationary (p-value: 0.0000)\n",
      "✅ SBER, return_2d: Already stationary (p-value: 0.0000)\n",
      "✅ SBER, return_3d: Already stationary (p-value: 0.0000)\n",
      "✅ SBER, return_4d: Already stationary (p-value: 0.0000)\n",
      "✅ SBER, return_5d: Already stationary (p-value: 0.0000)\n",
      "✅ SBER, return_6d: Already stationary (p-value: 0.0000)\n",
      "✅ SBER, return_7d: Already stationary (p-value: 0.0000)\n",
      "✅ SBER, return_8d: Already stationary (p-value: 0.0000)\n",
      "✅ SBER, return_9d: Already stationary (p-value: 0.0000)\n",
      "✅ SBER, return_10d: Already stationary (p-value: 0.0000)\n",
      "✅ SBER, return_11d: Already stationary (p-value: 0.0000)\n",
      "✅ SBER, return_12d: Already stationary (p-value: 0.0000)\n",
      "✅ SBER, return_13d: Already stationary (p-value: 0.0000)\n",
      "✅ SBER, return_14d: Already stationary (p-value: 0.0000)\n",
      "✅ SBER, return_15d: Already stationary (p-value: 0.0000)\n",
      "✅ SBER, return_16d: Already stationary (p-value: 0.0000)\n",
      "✅ SBER, return_17d: Already stationary (p-value: 0.0000)\n",
      "✅ SBER, return_18d: Already stationary (p-value: 0.0000)\n",
      "✅ SBER, return_19d: Already stationary (p-value: 0.0000)\n",
      "✅ SBER, return_20d: Already stationary (p-value: 0.0000)\n",
      "🔗 Adding compressed news embeddings to dataset...\n",
      "✅ Added 64 compressed news embeddings to 1691 rows\n",
      "📅 Filtered out 681 rows before 2022-05-01 00:00:00\n",
      "\n",
      "📊 Processing ticker: SIBN\n",
      "✅ SIBN, return_1d: Already stationary (p-value: 0.0000)\n",
      "✅ SIBN, return_2d: Already stationary (p-value: 0.0000)\n",
      "✅ SIBN, return_3d: Already stationary (p-value: 0.0000)\n",
      "✅ SIBN, return_4d: Already stationary (p-value: 0.0000)\n",
      "✅ SIBN, return_5d: Already stationary (p-value: 0.0000)\n",
      "✅ SIBN, return_6d: Already stationary (p-value: 0.0000)\n",
      "✅ SIBN, return_7d: Already stationary (p-value: 0.0000)\n",
      "✅ SIBN, return_8d: Already stationary (p-value: 0.0000)\n",
      "✅ SIBN, return_9d: Already stationary (p-value: 0.0000)\n",
      "✅ SIBN, return_10d: Already stationary (p-value: 0.0000)\n",
      "✅ SIBN, return_11d: Already stationary (p-value: 0.0000)\n",
      "✅ SIBN, return_12d: Already stationary (p-value: 0.0000)\n",
      "✅ SIBN, return_13d: Already stationary (p-value: 0.0000)\n",
      "✅ SIBN, return_14d: Already stationary (p-value: 0.0000)\n",
      "✅ SIBN, return_15d: Already stationary (p-value: 0.0000)\n",
      "✅ SIBN, return_16d: Already stationary (p-value: 0.0000)\n",
      "✅ SIBN, return_17d: Already stationary (p-value: 0.0000)\n",
      "✅ SIBN, return_18d: Already stationary (p-value: 0.0000)\n",
      "✅ SIBN, return_19d: Already stationary (p-value: 0.0000)\n",
      "✅ SIBN, return_20d: Already stationary (p-value: 0.0000)\n",
      "🔗 Adding compressed news embeddings to dataset...\n",
      "✅ Added 64 compressed news embeddings to 1691 rows\n",
      "📅 Filtered out 681 rows before 2022-05-01 00:00:00\n",
      "\n",
      "📊 Processing ticker: T\n",
      "✅ T, return_1d: Already stationary (p-value: 0.0000)\n",
      "✅ T, return_2d: Already stationary (p-value: 0.0000)\n",
      "✅ T, return_3d: Already stationary (p-value: 0.0000)\n",
      "✅ T, return_4d: Already stationary (p-value: 0.0000)\n",
      "✅ T, return_5d: Already stationary (p-value: 0.0000)\n",
      "✅ T, return_6d: Already stationary (p-value: 0.0000)\n",
      "✅ T, return_7d: Already stationary (p-value: 0.0000)\n",
      "✅ T, return_8d: Already stationary (p-value: 0.0000)\n",
      "✅ T, return_9d: Already stationary (p-value: 0.0000)\n",
      "✅ T, return_10d: Already stationary (p-value: 0.0000)\n",
      "✅ T, return_11d: Already stationary (p-value: 0.0000)\n",
      "✅ T, return_12d: Already stationary (p-value: 0.0000)\n",
      "✅ T, return_13d: Already stationary (p-value: 0.0000)\n",
      "✅ T, return_14d: Already stationary (p-value: 0.0000)\n",
      "✅ T, return_15d: Already stationary (p-value: 0.0000)\n",
      "✅ T, return_16d: Already stationary (p-value: 0.0000)\n",
      "✅ T, return_17d: Already stationary (p-value: 0.0000)\n",
      "✅ T, return_18d: Already stationary (p-value: 0.0000)\n",
      "✅ T, return_19d: Already stationary (p-value: 0.0000)\n",
      "✅ T, return_20d: Already stationary (p-value: 0.0000)\n",
      "🔗 Adding compressed news embeddings to dataset...\n",
      "✅ Added 64 compressed news embeddings to 1691 rows\n",
      "📅 Filtered out 681 rows before 2022-05-01 00:00:00\n",
      "\n",
      "📊 Processing ticker: VTBR\n",
      "✅ VTBR, return_1d: Already stationary (p-value: 0.0000)\n",
      "✅ VTBR, return_2d: Already stationary (p-value: 0.0000)\n",
      "✅ VTBR, return_3d: Already stationary (p-value: 0.0000)\n",
      "✅ VTBR, return_4d: Already stationary (p-value: 0.0000)\n",
      "✅ VTBR, return_5d: Already stationary (p-value: 0.0000)\n",
      "✅ VTBR, return_6d: Already stationary (p-value: 0.0000)\n",
      "✅ VTBR, return_7d: Already stationary (p-value: 0.0000)\n",
      "✅ VTBR, return_8d: Already stationary (p-value: 0.0000)\n",
      "✅ VTBR, return_9d: Already stationary (p-value: 0.0000)\n",
      "✅ VTBR, return_10d: Already stationary (p-value: 0.0000)\n",
      "✅ VTBR, return_11d: Already stationary (p-value: 0.0000)\n",
      "✅ VTBR, return_12d: Already stationary (p-value: 0.0000)\n",
      "✅ VTBR, return_13d: Already stationary (p-value: 0.0000)\n",
      "✅ VTBR, return_14d: Already stationary (p-value: 0.0000)\n",
      "✅ VTBR, return_15d: Already stationary (p-value: 0.0000)\n",
      "✅ VTBR, return_16d: Already stationary (p-value: 0.0000)\n",
      "✅ VTBR, return_17d: Already stationary (p-value: 0.0000)\n",
      "✅ VTBR, return_18d: Already stationary (p-value: 0.0000)\n",
      "✅ VTBR, return_19d: Already stationary (p-value: 0.0000)\n",
      "✅ VTBR, return_20d: Already stationary (p-value: 0.0000)\n",
      "🔗 Adding compressed news embeddings to dataset...\n",
      "✅ Added 64 compressed news embeddings to 1691 rows\n",
      "📅 Filtered out 681 rows before 2022-05-01 00:00:00\n",
      "\n",
      "✅ Processing complete! Created 113 features\n",
      "📈 Processed tickers: 19\n",
      "✅ Processed data: (23313, 135)\n",
      "\n",
      "🎯 Preparing training and prediction data...\n",
      "📊 Training data: (22933, 113)\n",
      "🎯 Targets: (22933, 20)\n",
      "🔮 Prediction data: (19, 113)\n",
      "📅 Training date range: 2022-05-01 to 2025-08-19\n",
      "📅 Prediction date: 2025-09-08\n",
      "\n",
      "🎯 Training final model...\n",
      "🎯 Training final CatBoost model...\n",
      "✅ Final model trained!\n",
      "   Train MAE: 0.030872\n",
      "\n",
      "🔮 Making predictions for the latest day...\n",
      "✅ Predictions ready for 19 tickers\n",
      "📊 Sample predictions:\n",
      "  ticker        p1        p2        p3        p4        p5        p6  \\\n",
      "0   AFLT -0.002961 -0.000595 -0.003237 -0.007187 -0.005445 -0.006327   \n",
      "1   ALRS -0.003732 -0.001745 -0.005142 -0.010114 -0.009432 -0.010936   \n",
      "2   CHMF -0.002140 -0.000199 -0.003274 -0.008250 -0.008222 -0.009985   \n",
      "3   GAZP -0.003378 -0.001607 -0.005304 -0.010871 -0.011073 -0.013055   \n",
      "4   GMKN -0.003152 -0.001526 -0.005125 -0.011014 -0.011577 -0.013794   \n",
      "\n",
      "         p7        p8        p9  ...       p11       p12       p13       p14  \\\n",
      "0 -0.006463 -0.004071 -0.004247  ... -0.011477 -0.012906 -0.014430 -0.015716   \n",
      "1 -0.011495 -0.009381 -0.010213  ... -0.019257 -0.021063 -0.023860 -0.025824   \n",
      "2 -0.010879 -0.010259 -0.011999  ... -0.022794 -0.024130 -0.026733 -0.028707   \n",
      "3 -0.014251 -0.013350 -0.015058  ... -0.025156 -0.027323 -0.030566 -0.033021   \n",
      "4 -0.015159 -0.014612 -0.017034  ... -0.029520 -0.031386 -0.035284 -0.038083   \n",
      "\n",
      "        p15       p16       p17       p18       p19       p20  \n",
      "0 -0.016743 -0.020514 -0.023045 -0.022615 -0.023210 -0.022559  \n",
      "1 -0.027845 -0.033302 -0.035697 -0.035777 -0.037518 -0.037157  \n",
      "2 -0.031147 -0.038660 -0.040264 -0.041384 -0.043923 -0.043102  \n",
      "3 -0.035693 -0.042254 -0.044978 -0.046182 -0.048569 -0.048609  \n",
      "4 -0.041365 -0.049882 -0.052559 -0.054133 -0.057494 -0.057553  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "\n",
      "💾 Saving submission to submission.csv...\n",
      "🎉 SUBMISSION CREATED SUCCESSFULLY!\n",
      "============================================================\n",
      "\n",
      "✅ SUBMISSION FILE READY: submission.csv\n",
      "🎯 Ready for competition!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from scipy import stats\n",
    "import warnings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class AdvancedFinancialPreprocessor:\n",
    "    \"\"\"\n",
    "    Advanced pipeline for financial data preprocessing for multi-ticker forecasting.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, target_periods=list(range(1, 21)), stationarity_alpha=0.05, \n",
    "                 use_news=True, embedding_dim=32):\n",
    "        self.target_periods = target_periods\n",
    "        self.stationarity_alpha = stationarity_alpha\n",
    "        self.state = {}\n",
    "        self.stationary_info = {}\n",
    "        self.stationary_params = {}\n",
    "        self.feature_columns = []\n",
    "        self.is_fitted = False\n",
    "        self.use_news = use_news\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.news_model = None\n",
    "        self.pca = None\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def _load_news_model(self):\n",
    "        \"\"\"Load news embedding model if not already loaded.\"\"\"\n",
    "        if self.news_model is None and self.use_news:\n",
    "            try:\n",
    "                print(\"🔄 Loading news embedding model...\")\n",
    "                self.news_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "                print(\"✅ News model loaded successfully!\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed to load news model: {e}\")\n",
    "                self.use_news = False\n",
    "    \n",
    "    def _process_news_data(self, news_df, cache_path=None):\n",
    "        \"\"\"Process news data and generate compressed embeddings.\"\"\"\n",
    "        if not self.use_news or news_df is None:\n",
    "            return {}\n",
    "            \n",
    "        self._load_news_model()\n",
    "        \n",
    "        print(\"📰 Processing news data with compressed embeddings...\")\n",
    "        \n",
    "        news_df = news_df.copy()\n",
    "        news_df['news_date'] = pd.to_datetime(news_df['publish_date']).dt.strftime('%Y-%m-%d')\n",
    "        news_df['combined_text'] = news_df['title'] + \". \" + news_df['publication'].fillna('')\n",
    "        \n",
    "        daily_news = news_df.groupby('news_date')['combined_text'].apply(\n",
    "            lambda x: ' '.join(x.dropna())\n",
    "        ).reset_index()\n",
    "        \n",
    "        embeddings_by_date = {}\n",
    "        all_embeddings = []\n",
    "        dates_list = []\n",
    "        \n",
    "        for idx, row in daily_news.iterrows():\n",
    "            date = row['news_date']\n",
    "            text = row['combined_text']\n",
    "            \n",
    "            if not text or len(text.strip()) < 10:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                embedding = self.news_model.encode(\n",
    "                    text,\n",
    "                    batch_size=32,\n",
    "                    show_progress_bar=False,\n",
    "                    normalize_embeddings=True\n",
    "                )\n",
    "                embeddings_by_date[date] = embedding\n",
    "                all_embeddings.append(embedding)\n",
    "                dates_list.append(date)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error encoding news for date {date}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if all_embeddings and len(all_embeddings) > self.embedding_dim:\n",
    "            print(f\"🔧 Compressing embeddings from {all_embeddings[0].shape[0]} to {self.embedding_dim} dimensions...\")\n",
    "            all_embeddings_array = np.array(all_embeddings)\n",
    "            \n",
    "            if self.pca is None:\n",
    "                self.pca = PCA(n_components=self.embedding_dim)\n",
    "                compressed_embeddings = self.pca.fit_transform(all_embeddings_array)\n",
    "            else:\n",
    "                compressed_embeddings = self.pca.transform(all_embeddings_array)\n",
    "            \n",
    "            for i, date in enumerate(dates_list):\n",
    "                embeddings_by_date[date] = compressed_embeddings[i]\n",
    "        \n",
    "        print(f\"✅ Generated compressed embeddings for {len(embeddings_by_date)} news dates\")\n",
    "        return embeddings_by_date\n",
    "    \n",
    "    def _add_news_embeddings(self, df, news_embeddings):\n",
    "        \"\"\"Add compressed news embeddings to the dataframe.\"\"\"\n",
    "        if not self.use_news or not news_embeddings:\n",
    "            return df\n",
    "            \n",
    "        print(\"🔗 Adding compressed news embeddings to dataset...\")\n",
    "        \n",
    "        df = df.copy()\n",
    "        df['date_str'] = pd.to_datetime(df['begin']).dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "        first_embedding = next(iter(news_embeddings.values()))\n",
    "        embedding_dim = len(first_embedding)\n",
    "        \n",
    "        news_columns = [f'news_emb_{i}' for i in range(embedding_dim)]\n",
    "        \n",
    "        for col in news_columns:\n",
    "            df[col] = 0.0\n",
    "        \n",
    "        embeddings_added = 0\n",
    "        for date_str, embedding in news_embeddings.items():\n",
    "            mask = df['date_str'] == date_str\n",
    "            if mask.any():\n",
    "                for i, value in enumerate(embedding):\n",
    "                    df.loc[mask, f'news_emb_{i}'] = value\n",
    "                embeddings_added += mask.sum()\n",
    "        \n",
    "        df = df.drop(columns=['date_str'])\n",
    "        \n",
    "        print(f\"✅ Added {embedding_dim} compressed news embeddings to {embeddings_added} rows\")\n",
    "        return df\n",
    "\n",
    "    def _create_complete_calendar(self, df, ticker):\n",
    "        \"\"\"Create complete calendar with forward fill for missing dates.\"\"\"\n",
    "        df = df.copy()\n",
    "        df['date'] = pd.to_datetime(df['begin'])\n",
    "        \n",
    "        start_date = df['date'].min()\n",
    "        end_date = df['date'].max()\n",
    "        full_calendar = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "        \n",
    "        calendar_df = pd.DataFrame({'date': full_calendar})\n",
    "        calendar_df['ticker'] = ticker\n",
    "        calendar_df['begin'] = calendar_df['date'].dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "        df_for_merge = df.drop(columns=['date'], errors='ignore')\n",
    "        \n",
    "        merged_df = calendar_df.merge(\n",
    "            df_for_merge, on=['begin', 'ticker'], how='left', suffixes=('_calendar', '_data')\n",
    "        )\n",
    "        \n",
    "        price_columns = ['open', 'high', 'low', 'close', 'volume']\n",
    "        for col in price_columns:\n",
    "            if col in merged_df.columns:\n",
    "                merged_df[col] = merged_df[col].fillna(method='ffill')\n",
    "        \n",
    "        for col in price_columns:\n",
    "            if col in merged_df.columns:\n",
    "                merged_df[col] = merged_df[col].fillna(method='bfill')\n",
    "        \n",
    "        merged_df['is_original'] = ~merged_df['close'].isna()\n",
    "        \n",
    "        return merged_df\n",
    "\n",
    "    def _calculate_returns_targets(self, df, ticker):\n",
    "        \"\"\"Calculate future returns targets: r_(t+N) = close_(t+N)/close_t - 1.\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        original_days = df[df['is_original'] == True].copy()\n",
    "        original_days = original_days.sort_values('begin')\n",
    "        \n",
    "        if len(original_days) == 0:\n",
    "            print(f\"⚠️ No original days found for ticker {ticker}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        dates_list = original_days['begin'].tolist()\n",
    "        close_prices = original_days['close'].tolist()\n",
    "        \n",
    "        targets_data = []\n",
    "        \n",
    "        for idx, current_date in enumerate(dates_list):\n",
    "            row_targets = {'begin': current_date, 'ticker': ticker}\n",
    "            current_close = close_prices[idx]\n",
    "            \n",
    "            for N in self.target_periods:\n",
    "                future_idx = idx + N\n",
    "                if future_idx < len(dates_list):\n",
    "                    future_close = close_prices[future_idx]\n",
    "                    return_value = (future_close / current_close) - 1\n",
    "                    row_targets[f'return_{N}d'] = return_value\n",
    "                else:\n",
    "                    row_targets[f'return_{N}d'] = np.nan\n",
    "            \n",
    "            targets_data.append(row_targets)\n",
    "        \n",
    "        targets_df = pd.DataFrame(targets_data)\n",
    "        targets_df = targets_df.drop_duplicates(subset=['begin', 'ticker'], keep='last')\n",
    "        return targets_df\n",
    "\n",
    "    def _check_stationarity(self, series, max_lags=None):\n",
    "        \"\"\"Enhanced stationarity check handling all non-finite values.\"\"\"\n",
    "        series_array = np.asarray(series)\n",
    "        finite_mask = np.isfinite(series_array)\n",
    "        \n",
    "        if np.sum(finite_mask) < 10:\n",
    "            return False, 1.0\n",
    "        \n",
    "        series_clean = series_array[finite_mask]\n",
    "        \n",
    "        try:\n",
    "            adf_result = adfuller(series_clean, maxlag=max_lags, autolag='AIC')\n",
    "            p_value = adf_result[1]\n",
    "            return p_value < self.stationarity_alpha, p_value\n",
    "        except Exception as e:\n",
    "            print(f\"ADF test failed: {e}\")\n",
    "            return False, 1.0\n",
    "\n",
    "    def _apply_returns_stationarity_correction(self, df, ticker):\n",
    "        \"\"\"Apply stationarity correction to returns targets using safe methods.\"\"\"\n",
    "        df = df.copy()\n",
    "        stationary_targets = {}\n",
    "        \n",
    "        for N in self.target_periods:\n",
    "            target_col = f'return_{N}d'\n",
    "            if target_col not in df.columns:\n",
    "                continue\n",
    "                \n",
    "            original_mask = (df['is_original'] == True) & (df[target_col].notna())\n",
    "            original_series = df.loc[original_mask, target_col]\n",
    "            \n",
    "            if len(original_series) < 10:\n",
    "                print(f\"⚠️ Not enough data for {target_col}: {len(original_series)} points\")\n",
    "                df[f'stationary_return_{N}d'] = df[target_col]\n",
    "                stationary_targets[target_col] = ('none', 1.0)\n",
    "                continue\n",
    "            \n",
    "            is_stationary, p_value = self._check_stationarity(original_series)\n",
    "            \n",
    "            if not is_stationary:\n",
    "                print(f\"🔄 Applying stationarity correction for {target_col} (p-value: {p_value:.4f})\")\n",
    "                \n",
    "                log_stationary, log_pvalue = False, 1.0\n",
    "                log_series = None\n",
    "                if (original_series > -1).all():\n",
    "                    log_series = np.log1p(original_series)\n",
    "                    log_stationary, log_pvalue = self._check_stationarity(log_series)\n",
    "                \n",
    "                diff_series = original_series.diff().dropna()\n",
    "                diff_stationary, diff_pvalue = self._check_stationarity(diff_series) if len(diff_series) > 0 else (False, 1.0)\n",
    "                \n",
    "                methods = [\n",
    "                    ('log', log_stationary, log_pvalue, log_series),\n",
    "                    ('diff', diff_stationary, diff_pvalue, diff_series),\n",
    "                ]\n",
    "                \n",
    "                valid_methods = [(name, stat, pval, data) for name, stat, pval, data in methods \n",
    "                               if data is not None and len(data) > 0 and stat]\n",
    "                \n",
    "                if valid_methods:\n",
    "                    best_method = min(valid_methods, key=lambda x: x[2])\n",
    "                    method_name, _, best_pvalue, best_data = best_method\n",
    "                    \n",
    "                    if method_name == 'log':\n",
    "                        df[f'stationary_return_{N}d'] = np.log1p(df[target_col])\n",
    "                        self.stationary_params[f'{ticker}_return_{N}d'] = {\n",
    "                            'method': 'log',\n",
    "                            'original_mean': original_series.mean(),\n",
    "                            'original_std': original_series.std()\n",
    "                        }\n",
    "                    elif method_name == 'diff':\n",
    "                        df[f'stationary_return_{N}d'] = df[target_col].diff()\n",
    "                        first_value = original_series.iloc[0] if len(original_series) > 0 else 0\n",
    "                        self.stationary_params[f'{ticker}_return_{N}d'] = {\n",
    "                            'method': 'diff',\n",
    "                            'first_value': first_value\n",
    "                        }\n",
    "                    \n",
    "                    stationary_targets[target_col] = (method_name, best_pvalue)\n",
    "                    print(f\"✅ {ticker}, {target_col}: {method_name} applied (p-value: {best_pvalue:.4f})\")\n",
    "                else:\n",
    "                    stationary_targets[target_col] = ('none', p_value)\n",
    "                    df[f'stationary_return_{N}d'] = df[target_col]\n",
    "                    self.stationary_params[f'{ticker}_return_{N}d'] = {'method': 'none'}\n",
    "                    print(f\"❌ {ticker}, {target_col}: No effective transformation found (p-value: {p_value:.4f})\")\n",
    "            else:\n",
    "                stationary_targets[target_col] = ('none', p_value)\n",
    "                df[f'stationary_return_{N}d'] = df[target_col]\n",
    "                self.stationary_params[f'{ticker}_return_{N}d'] = {'method': 'none'}\n",
    "                print(f\"✅ {ticker}, {target_col}: Already stationary (p-value: {p_value:.4f})\")\n",
    "        \n",
    "        return df, stationary_targets\n",
    "\n",
    "    def _inverse_transform_predictions(self, predictions, ticker, horizon):\n",
    "        \"\"\"Inverse transform predictions from stationary space back to returns.\"\"\"\n",
    "        key = f'{ticker}_return_{horizon}d'\n",
    "        \n",
    "        if key not in self.stationary_params:\n",
    "            return predictions\n",
    "        \n",
    "        params = self.stationary_params[key]\n",
    "        method = params.get('method', 'none')\n",
    "        \n",
    "        if method == 'log':\n",
    "            return np.exp(predictions) - 1\n",
    "        elif method == 'diff':\n",
    "            print(f\"⚠️ Differencing inverse transform requires last value - returning as is for {ticker} {horizon}d\")\n",
    "            return predictions\n",
    "        else:\n",
    "            return predictions\n",
    "\n",
    "    def _create_leak_proof_features(self, df):\n",
    "        \"\"\"Create features using ONLY historical data (no future leakage).\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        required_columns = ['open', 'high', 'low', 'close', 'volume']\n",
    "        if not all(col in df.columns for col in required_columns):\n",
    "            print(\"⚠️ Missing required price columns\")\n",
    "            return df\n",
    "        \n",
    "        # Basic price transformations\n",
    "        df['hl_range'] = (df['high'] - df['low']) / df['close']\n",
    "        df['oc_range'] = (df['close'] - df['open']) / df['open']\n",
    "        df['price_change'] = df['close'].pct_change()\n",
    "        \n",
    "        # Moving averages (historical only)\n",
    "        for window in [5, 10, 20]:\n",
    "            df[f'sma_{window}'] = df['close'].rolling(window, min_periods=1).mean()\n",
    "            df[f'ema_{window}'] = df['close'].ewm(span=window, min_periods=1).mean()\n",
    "        \n",
    "        # Volatility (historical)\n",
    "        for window in [5, 10, 20]:\n",
    "            df[f'volatility_{window}'] = df['price_change'].rolling(window, min_periods=1).std()\n",
    "        \n",
    "        # RSI (historical)\n",
    "        def compute_rsi(series, window=14):\n",
    "            delta = series.diff()\n",
    "            gain = (delta.where(delta > 0, 0)).rolling(window, min_periods=1).mean()\n",
    "            loss = (-delta.where(delta < 0, 0)).rolling(window, min_periods=1).mean()\n",
    "            rs = gain / loss\n",
    "            rsi = 100 - (100 / (1 + rs))\n",
    "            return rsi.replace([np.inf, -np.inf], np.nan).fillna(50)\n",
    "        \n",
    "        df['rsi_14'] = compute_rsi(df['close'])\n",
    "        \n",
    "        # MACD (historical)\n",
    "        exp1 = df['close'].ewm(span=12, min_periods=1).mean()\n",
    "        exp2 = df['close'].ewm(span=26, min_periods=1).mean()\n",
    "        df['macd'] = exp1 - exp2\n",
    "        df['macd_signal'] = df['macd'].ewm(span=9, min_periods=1).mean()\n",
    "        \n",
    "        # Lag features (historical only)\n",
    "        for lag in [1, 2, 3, 5, 7]:\n",
    "            df[f'close_lag_{lag}'] = df['close'].shift(lag)\n",
    "            df[f'volume_lag_{lag}'] = df['volume'].shift(lag)\n",
    "            df[f'price_change_lag_{lag}'] = df['price_change'].shift(lag)\n",
    "        \n",
    "        # Rolling statistics\n",
    "        for window in [5, 10]:\n",
    "            df[f'close_rolling_mean_{window}'] = df['close'].rolling(window, min_periods=1).mean()\n",
    "            df[f'close_rolling_std_{window}'] = df['close'].rolling(window, min_periods=1).std()\n",
    "            df[f'volume_rolling_mean_{window}'] = df['volume'].rolling(window, min_periods=1).mean()\n",
    "        \n",
    "        # Temporal features\n",
    "        if 'date' not in df.columns:\n",
    "            df['date'] = pd.to_datetime(df['begin'])\n",
    "        df['day_of_week'] = df['date'].dt.dayofweek\n",
    "        df['month'] = df['date'].dt.month\n",
    "        df['quarter'] = df['date'].dt.quarter\n",
    "        df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "        df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "        df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "        df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "        \n",
    "        # Fill any remaining NaN values\n",
    "        numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "        df[numeric_columns] = df[numeric_columns].fillna(method='bfill').fillna(method='ffill')\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def _clean_target_columns(self, df):\n",
    "        \"\"\"Remove original target columns, keep only stationary versions.\"\"\"\n",
    "        original_targets = [f'return_{N}d' for N in self.target_periods]\n",
    "        columns_to_keep = [col for col in df.columns if col not in original_targets]\n",
    "        return df[columns_to_keep]\n",
    "\n",
    "    def _safe_merge(self, left, right, on, how='left'):\n",
    "        \"\"\"Safe merge with duplicate handling and validation.\"\"\"\n",
    "        left_clean = left.drop_duplicates(subset=on, keep='last')\n",
    "        right_clean = right.drop_duplicates(subset=on, keep='last')\n",
    "        \n",
    "        try:\n",
    "            merged = left_clean.merge(right_clean, on=on, how=how, validate='one_to_one')\n",
    "            return merged\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Merge validation failed: {e}, using safe merge without validation\")\n",
    "            return left_clean.merge(right_clean, on=on, how=how)\n",
    "\n",
    "    def _filter_data_by_date(self, df, min_date='2022-05-01'):\n",
    "        \"\"\"Filter out data before May 2022.\"\"\"\n",
    "        if df.empty:\n",
    "            return df\n",
    "            \n",
    "        df = df.copy()\n",
    "        df['date'] = pd.to_datetime(df['begin'])\n",
    "        min_date = pd.to_datetime(min_date)\n",
    "        \n",
    "        initial_count = len(df)\n",
    "        df = df[df['date'] >= min_date].copy()\n",
    "        filtered_count = len(df)\n",
    "        \n",
    "        if initial_count > filtered_count:\n",
    "            print(f\"📅 Filtered out {initial_count - filtered_count} rows before {min_date}\")\n",
    "        \n",
    "        if 'date' in df.columns and 'begin' in df.columns:\n",
    "            df = df.drop(columns=['date'])\n",
    "            \n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, df, news_df=None, calculate_targets=True, news_cache_path=None):\n",
    "        \"\"\"Process training data for returns prediction.\"\"\"\n",
    "        print(\"🚀 Processing training data for returns prediction...\")\n",
    "        \n",
    "        if df.empty:\n",
    "            raise ValueError(\"Input DataFrame is empty\")\n",
    "        \n",
    "        required_columns = ['begin', 'ticker', 'open', 'high', 'low', 'close', 'volume']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "        \n",
    "        # Process news data\n",
    "        news_embeddings = {}\n",
    "        if self.use_news and news_df is not None:\n",
    "            news_embeddings = self._process_news_data(news_df, cache_path=news_cache_path)\n",
    "        \n",
    "        processed_dfs = []\n",
    "        \n",
    "        for ticker in df['ticker'].unique():\n",
    "            print(f\"\\n📊 Processing ticker: {ticker}\")\n",
    "            \n",
    "            ticker_data = df[df['ticker'] == ticker].copy().sort_values('begin')\n",
    "            \n",
    "            if len(ticker_data) == 0:\n",
    "                print(f\"⚠️ No data for ticker {ticker}, skipping\")\n",
    "                continue\n",
    "            \n",
    "            # Create complete calendar\n",
    "            regular_data = self._create_complete_calendar(ticker_data, ticker)\n",
    "            \n",
    "            # Calculate returns targets\n",
    "            if calculate_targets:\n",
    "                targets_df = self._calculate_returns_targets(regular_data, ticker)\n",
    "                if len(targets_df) > 0:\n",
    "                    regular_data = self._safe_merge(regular_data, targets_df, on=['begin', 'ticker'])\n",
    "                    \n",
    "                    # Apply stationarity correction\n",
    "                    regular_data, stationary_targets = self._apply_returns_stationarity_correction(regular_data, ticker)\n",
    "                    self.stationary_info[ticker] = stationary_targets\n",
    "                else:\n",
    "                    print(f\"⚠️ No targets calculated for {ticker}\")\n",
    "            \n",
    "            # Create leak-proof features\n",
    "            regular_data = self._create_leak_proof_features(regular_data)\n",
    "            \n",
    "            # Add news embeddings\n",
    "            if self.use_news:\n",
    "                regular_data = self._add_news_embeddings(regular_data, news_embeddings)\n",
    "            \n",
    "            # Clean target columns - keep only stationary versions\n",
    "            if calculate_targets:\n",
    "                regular_data = self._clean_target_columns(regular_data)\n",
    "            \n",
    "            # Filter data before May 2022\n",
    "            regular_data = self._filter_data_by_date(regular_data, '2022-05-01')\n",
    "            \n",
    "            # Keep ticker as string for CatBoost\n",
    "            regular_data['ticker'] = ticker\n",
    "            \n",
    "            # Save state\n",
    "            self.state[ticker] = {\n",
    "                'last_data': regular_data.copy(),\n",
    "                'last_date': regular_data['begin'].max()\n",
    "            }\n",
    "            \n",
    "            processed_dfs.append(regular_data)\n",
    "        \n",
    "        if not processed_dfs:\n",
    "            raise ValueError(\"No data processed - check input data\")\n",
    "        \n",
    "        # Combine all data\n",
    "        final_df = pd.concat(processed_dfs, ignore_index=True)\n",
    "        final_df = final_df.sort_values(['begin', 'ticker']).reset_index(drop=True)\n",
    "        \n",
    "        # Define feature columns (ВКЛЮЧАЕМ 'ticker')\n",
    "        self.feature_columns = [\n",
    "            col for col in final_df.columns \n",
    "            if not col.startswith(('return_', 'stationary_return_', 'target_')) \n",
    "            and col not in ['begin', 'date', 'is_original', 'news_date']\n",
    "        ]\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        print(f\"\\n✅ Processing complete! Created {len(self.feature_columns)} features\")\n",
    "        print(f\"📈 Processed tickers: {len(processed_dfs)}\")\n",
    "        \n",
    "        return final_df\n",
    "\n",
    "    def transform(self, df, news_df=None, calculate_targets=False, news_cache_path=None):\n",
    "        \"\"\"Process new data incrementally for inference.\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Preprocessor must be fitted first!\")\n",
    "        \n",
    "        print(\"🔄 Incremental processing of new data...\")\n",
    "        \n",
    "        news_embeddings = {}\n",
    "        if self.use_news and news_df is not None:\n",
    "            news_embeddings = self._process_news_data(news_df, cache_path=news_cache_path)\n",
    "        \n",
    "        processed_dfs = []\n",
    "        \n",
    "        for ticker in df['ticker'].unique():\n",
    "            if ticker not in self.state:\n",
    "                print(f\"⚠️ Skipping unknown ticker: {ticker}\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"📊 Processing ticker: {ticker}\")\n",
    "            \n",
    "            new_ticker_data = df[df['ticker'] == ticker].copy().sort_values('begin')\n",
    "            saved_state = self.state[ticker]\n",
    "            last_data = saved_state['last_data']\n",
    "            \n",
    "            new_ticker_data = self._filter_data_by_date(new_ticker_data, '2022-05-01')\n",
    "            \n",
    "            last_known_date = pd.to_datetime(saved_state['last_date'])\n",
    "            new_ticker_data_dates = pd.to_datetime(new_ticker_data['begin'])\n",
    "            new_ticker_data = new_ticker_data[new_ticker_data_dates > last_known_date].copy()\n",
    "            \n",
    "            if len(new_ticker_data) == 0:\n",
    "                print(f\"⚠️ No new data for {ticker}\")\n",
    "                continue\n",
    "            \n",
    "            combined_data = pd.concat([last_data, new_ticker_data], ignore_index=True)\n",
    "            combined_data = combined_data.drop_duplicates(\n",
    "                subset=['begin', 'ticker'], keep='last'\n",
    "            ).sort_values('begin').reset_index(drop=True)\n",
    "            \n",
    "            regular_data = self._create_complete_calendar(combined_data, ticker)\n",
    "            regular_data = self._create_leak_proof_features(regular_data)\n",
    "            \n",
    "            if self.use_news:\n",
    "                regular_data = self._add_news_embeddings(regular_data, news_embeddings)\n",
    "            \n",
    "            final_ticker_data = regular_data[\n",
    "                pd.to_datetime(regular_data['begin']) > last_known_date\n",
    "            ].copy()\n",
    "            \n",
    "            final_ticker_data = self._filter_data_by_date(final_ticker_data, '2022-05-01')\n",
    "            final_ticker_data['ticker'] = ticker\n",
    "            \n",
    "            self.state[ticker] = {\n",
    "                'last_data': regular_data.copy(),\n",
    "                'last_date': regular_data['begin'].max()\n",
    "            }\n",
    "            \n",
    "            processed_dfs.append(final_ticker_data)\n",
    "        \n",
    "        if not processed_dfs:\n",
    "            print(\"⚠️ No new data processed\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        final_df = pd.concat(processed_dfs, ignore_index=True)\n",
    "        final_df = final_df.sort_values(['begin', 'ticker']).reset_index(drop=True)\n",
    "        \n",
    "        print(f\"✅ New data processed! Size: {final_df.shape}\")\n",
    "        return final_df\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        \"\"\"Get list of feature names.\"\"\"\n",
    "        return self.feature_columns.copy()\n",
    "\n",
    "    def get_returns_target_names(self):\n",
    "        \"\"\"Get list of stationary returns target names.\"\"\"\n",
    "        return [f'stationary_return_{N}d' for N in self.target_periods]\n",
    "\n",
    "    def get_final_predictions_format(self, predictions, tickers):\n",
    "        \"\"\"Convert model predictions to final competition format with inverse transformation.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i, ticker in enumerate(tickers):\n",
    "            row = {'ticker': ticker}\n",
    "            for j, N in enumerate(self.target_periods):\n",
    "                if hasattr(predictions, 'shape') and len(predictions.shape) > 1:\n",
    "                    pred_value = predictions[i, j]\n",
    "                else:\n",
    "                    pred_value = predictions[j] if i == 0 else predictions[j + i * len(self.target_periods)]\n",
    "                \n",
    "                inverse_pred = self._inverse_transform_predictions(pred_value, ticker, N)\n",
    "                row[f'p{N}'] = inverse_pred\n",
    "            \n",
    "            results.append(row)\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "def train_final_catboost_model(X_train, y_train, categorical_features=['ticker']):\n",
    "    \"\"\"\n",
    "    Train final CatBoost model for competition.\n",
    "    \"\"\"\n",
    "    from catboost import CatBoostRegressor\n",
    "    import numpy as np\n",
    "    \n",
    "    print(\"🎯 Training final CatBoost model...\")\n",
    "    \n",
    "    cat_features_indices = [i for i, col in enumerate(X_train.columns) if col in categorical_features]\n",
    "    \n",
    "    model = CatBoostRegressor(\n",
    "        iterations=300,\n",
    "        learning_rate=0.02,\n",
    "        depth=8,\n",
    "        loss_function='MultiRMSE',\n",
    "        random_seed=42,\n",
    "        verbose=100,\n",
    "        early_stopping_rounds=10,\n",
    "        thread_count=-1\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        cat_features=cat_features_indices,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    train_pred = model.predict(X_train)\n",
    "    train_mae = np.mean(np.abs(train_pred - y_train.values))\n",
    "    \n",
    "    print(f\"✅ Final model trained!\")\n",
    "    print(f\"   Train MAE: {train_mae:.6f}\")\n",
    "    \n",
    "    return model, train_pred\n",
    "\n",
    "def prepare_competition_submission(train_candles_path, train_news_path, output_path=\"submission.csv\"):\n",
    "    \"\"\"\n",
    "    Complete pipeline for competition submission.\n",
    "    \"\"\"\n",
    "    print(\"🚀 PREPARING COMPETITION SUBMISSION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Load data\n",
    "    print(\"📥 Loading training data...\")\n",
    "    train_data = pd.read_csv(train_candles_path)\n",
    "    train_news = pd.read_csv(train_news_path)\n",
    "    \n",
    "    print(f\"✅ Train candles: {train_data.shape}\")\n",
    "    print(f\"✅ Train news: {train_news.shape}\")\n",
    "    print(f\"📅 Date range: {train_data['begin'].min()} to {train_data['begin'].max()}\")\n",
    "    print(f\"🎯 Tickers: {train_data['ticker'].unique()}\")\n",
    "    \n",
    "    # 2. Initialize and process data\n",
    "    print(\"\\n🔄 Processing data with preprocessor...\")\n",
    "    preprocessor = AdvancedFinancialPreprocessor(\n",
    "        target_periods=list(range(1, 21)),\n",
    "        use_news=True,\n",
    "        embedding_dim=64\n",
    "    )\n",
    "    \n",
    "    processed_data = preprocessor.fit_transform(\n",
    "        train_data, \n",
    "        news_df=train_news,\n",
    "        calculate_targets=True\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Processed data: {processed_data.shape}\")\n",
    "    \n",
    "    # 3. Prepare training and prediction data\n",
    "    print(\"\\n🎯 Preparing training and prediction data...\")\n",
    "    \n",
    "    feature_names = preprocessor.get_feature_names()\n",
    "    target_names = preprocessor.get_returns_target_names()\n",
    "    \n",
    "    # Sort by date\n",
    "    processed_data = processed_data.sort_values('begin').reset_index(drop=True)\n",
    "    unique_dates = processed_data['begin'].unique()\n",
    "    \n",
    "    # Remove last 20 days where targets are incomplete for training\n",
    "    train_cutoff_date = unique_dates[-21] if len(unique_dates) > 20 else unique_dates[0]\n",
    "    \n",
    "    # Training data: exclude last 20 days\n",
    "    train_mask = processed_data['begin'] <= train_cutoff_date\n",
    "    # Prediction data: latest available day for each ticker\n",
    "    latest_data = processed_data.groupby('ticker').last().reset_index()\n",
    "    \n",
    "    X_train = processed_data.loc[train_mask, feature_names]\n",
    "    y_train = processed_data.loc[train_mask, target_names]\n",
    "    X_pred = latest_data[feature_names]\n",
    "    \n",
    "    print(f\"📊 Training data: {X_train.shape}\")\n",
    "    print(f\"🎯 Targets: {y_train.shape}\")\n",
    "    print(f\"🔮 Prediction data: {X_pred.shape}\")\n",
    "    print(f\"📅 Training date range: {processed_data.loc[train_mask, 'begin'].min()} to {processed_data.loc[train_mask, 'begin'].max()}\")\n",
    "    print(f\"📅 Prediction date: {latest_data['begin'].max()}\")\n",
    "    \n",
    "    # 4. Train final model\n",
    "    print(\"\\n🎯 Training final model...\")\n",
    "    \n",
    "    if 'ticker' not in X_train.columns:\n",
    "        print(\"❌ ERROR: 'ticker' column missing from features!\")\n",
    "        return None\n",
    "        \n",
    "    model, train_pred = train_final_catboost_model(X_train, y_train)\n",
    "    \n",
    "    # 5. Make predictions\n",
    "    print(\"\\n🔮 Making predictions for the latest day...\")\n",
    "    \n",
    "    if X_pred.empty:\n",
    "        print(\"❌ No data for prediction!\")\n",
    "        return None\n",
    "        \n",
    "    predictions = model.predict(X_pred)\n",
    "    prediction_tickers = latest_data['ticker'].tolist()\n",
    "    \n",
    "    final_predictions = preprocessor.get_final_predictions_format(predictions, prediction_tickers)\n",
    "    \n",
    "    print(f\"✅ Predictions ready for {len(final_predictions)} tickers\")\n",
    "    print(\"📊 Sample predictions:\")\n",
    "    print(final_predictions.head())\n",
    "    \n",
    "    # 6. Save submission\n",
    "    print(f\"\\n💾 Saving submission to {output_path}...\")\n",
    "    final_predictions.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(\"🎉 SUBMISSION CREATED SUCCESSFULLY!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return {\n",
    "        'preprocessor': preprocessor,\n",
    "        'model': model,\n",
    "        'predictions': final_predictions,\n",
    "        'processed_data': processed_data\n",
    "    }\n",
    "\n",
    "def run_simple_competition_pipeline(train_candles_path, train_news_path, output_path=\"submission.csv\"):\n",
    "    \"\"\"\n",
    "    Simplified pipeline for quick submission.\n",
    "    \"\"\"\n",
    "    print(\"🚀 SIMPLE COMPETITION PIPELINE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load data\n",
    "    train_data = pd.read_csv(train_candles_path)\n",
    "    train_news = pd.read_csv(train_news_path)\n",
    "    \n",
    "    print(f\"📊 Loaded {len(train_data)} rows, {train_data['ticker'].nunique()} tickers\")\n",
    "    \n",
    "    # Process data\n",
    "    preprocessor = AdvancedFinancialPreprocessor(\n",
    "        target_periods=list(range(1, 21)),\n",
    "        use_news=True,\n",
    "        embedding_dim=16\n",
    "    )\n",
    "    \n",
    "    processed_data = preprocessor.fit_transform(\n",
    "        train_data, \n",
    "        news_df=train_news,\n",
    "        calculate_targets=True\n",
    "    )\n",
    "    \n",
    "    # Get latest data for each ticker\n",
    "    latest_data = processed_data.sort_values('begin').groupby('ticker').last().reset_index()\n",
    "    \n",
    "    feature_names = preprocessor.get_feature_names()\n",
    "    target_names = preprocessor.get_returns_target_names()\n",
    "    \n",
    "    # Prepare training data (exclude last 20 days)\n",
    "    train_cutoff = processed_data['begin'].max()\n",
    "    train_data_filtered = processed_data[processed_data['begin'] < train_cutoff]\n",
    "    \n",
    "    if train_data_filtered.empty:\n",
    "        train_data_filtered = processed_data.iloc[:-20]\n",
    "    \n",
    "    X_train = train_data_filtered[feature_names]\n",
    "    y_train = train_data_filtered[target_names]\n",
    "    X_pred = latest_data[feature_names]\n",
    "    \n",
    "    print(f\"🎯 Training on {X_train.shape[0]} samples\")\n",
    "    print(f\"🔮 Predicting for {X_pred.shape[0]} tickers\")\n",
    "    \n",
    "    # Train model\n",
    "    from catboost import CatBoostRegressor\n",
    "    \n",
    "    model = CatBoostRegressor(\n",
    "        iterations=300,\n",
    "        learning_rate=0.02,\n",
    "        depth=8,\n",
    "        loss_function='MultiRMSE',\n",
    "        random_seed=42,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    cat_features_indices = [i for i, col in enumerate(X_train.columns) if col == 'ticker']\n",
    "    model.fit(X_train, y_train, cat_features=cat_features_indices, verbose=False)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_pred)\n",
    "    final_predictions = preprocessor.get_final_predictions_format(\n",
    "        predictions, latest_data['ticker'].tolist()\n",
    "    )\n",
    "    \n",
    "    # Save\n",
    "    final_predictions.to_csv(output_path, index=False)\n",
    "    print(f\"✅ Submission saved to {output_path}\")\n",
    "    print(\"📊 First few predictions:\")\n",
    "    print(final_predictions.head())\n",
    "    \n",
    "    return final_predictions\n",
    "\n",
    "# MAIN EXECUTION\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 FINANCIAL FORECASTING COMPETITION PIPELINE\")\n",
    "    print(\"Version: Final Competition Ready\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Use one of these options:\n",
    "    \n",
    "    # Option 1: Full pipeline (recommended)\n",
    "    submission_result = prepare_competition_submission(\n",
    "        train_candles_path=\"../forecast_data/candles_full.csv\",  # Update path\n",
    "        train_news_path=\"../forecast_data/news_full.csv\",        # Update path  \n",
    "        output_path=\"submission.csv\"\n",
    "    )\n",
    "    \n",
    "    # Option 2: Simple pipeline (if full has issues)\n",
    "    # submission_result = run_simple_competition_pipeline(\n",
    "    #     train_candles_path=\"../forecast_data/candles.csv\",\n",
    "    #     train_news_path=\"../forecast_data/news.csv\",\n",
    "    #     output_path=\"submission.csv\"\n",
    "    # )\n",
    "    \n",
    "    if submission_result:\n",
    "        print(\"\\n✅ SUBMISSION FILE READY: submission.csv\")\n",
    "        print(\"🎯 Ready for competition!\")\n",
    "    else:\n",
    "        print(\"\\n❌ SUBMISSION FAILED!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ca52c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Объединение завершено! Проверьте файлы: candles_full.csv и news_full.csv\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Объединение свечных данных\n",
    "# train_candles = pd.read_csv('../forecast_data/candles.csv')\n",
    "# test_candles = pd.read_csv('../forecast_data/candles_2.csv')\n",
    "# candles_full = pd.concat([train_candles, test_candles], ignore_index=True)\n",
    "# candles_full.to_csv('../forecast_data/candles_full.csv', index=False)\n",
    "\n",
    "# # Объединение новостных данных\n",
    "# train_news = pd.read_csv('../forecast_data/news.csv')\n",
    "# test_news = pd.read_csv('../forecast_data/news_2.csv')\n",
    "# news_full = pd.concat([train_news, test_news], ignore_index=True)\n",
    "# news_full.to_csv('../forecast_data/news_full.csv', index=False)\n",
    "\n",
    "# print(\"Объединение завершено! Проверьте файлы: candles_full.csv и news_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991f41a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
